{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d27413a",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iwf6njp9rtd",
   "source": "# Week 2 / Video 6: Reranking\n\n## üéØ Learning Objectives\n\nThis notebook demonstrates **two-stage retrieval** using reranking to improve search relevance:\n\n1. **Stage 1 - Initial Retrieval**: Fast hybrid search (dense + sparse) retrieves broad candidate set (k=20)\n2. **Stage 2 - Reranking**: Slower but more accurate cross-encoder model reorders candidates by relevance\n\n## üîë Key Concepts\n\n### Why Reranking?\n\n**Problem**: Embedding models (bi-encoders) are fast but have limited accuracy:\n- Query and documents are encoded independently\n- Similarity is just dot product of vectors\n- No direct interaction between query and document tokens\n- Good for initial retrieval, but not optimal for final ranking\n\n**Solution**: Reranking models (cross-encoders) are slower but more accurate:\n- Query and document are encoded together\n- Model can see relationships between query and document tokens\n- Much better at understanding semantic relevance\n- Too slow for full corpus search, but perfect for refining top-K results\n\n### Bi-Encoder vs Cross-Encoder\n\n**Bi-Encoder (Retrieval Model)**:\n```\nQuery ‚Üí Encoder ‚Üí [0.1, 0.5, 0.8, ...]\nDocument ‚Üí Encoder ‚Üí [0.2, 0.4, 0.9, ...]\nSimilarity = dot_product(query_vec, doc_vec)\n```\n- ‚úÖ **Fast**: Pre-computed document embeddings, simple dot product\n- ‚úÖ **Scalable**: Can search millions of documents in milliseconds\n- ‚ùå **Limited accuracy**: No query-document interaction\n\n**Cross-Encoder (Reranking Model)**:\n```\n[Query, Document] ‚Üí Encoder ‚Üí Relevance Score (0-1)\n```\n- ‚úÖ **High accuracy**: Full attention between query and document tokens\n- ‚úÖ **Better semantic understanding**: Can identify nuanced relevance\n- ‚ùå **Slow**: Must re-encode every query-document pair (N forward passes)\n- ‚ùå **Not scalable**: Can't pre-compute, must run on-demand\n\n### Two-Stage Retrieval Pipeline\n\n```\nUser Query\n    ‚Üì\nStage 1: Hybrid Search (Bi-Encoder)\n  - Dense: text-embedding-3-small (semantic)\n  - Sparse: BM25 (keyword matching)\n  - Fusion: RRF (Reciprocal Rank Fusion)\n  - Result: Top 20 candidates (~100ms)\n    ‚Üì\nStage 2: Reranking (Cross-Encoder)\n  - Model: Cohere rerank-v4.0-pro\n  - Input: Query + Top 20 documents\n  - Output: Reordered results with relevance scores\n  - Result: Top 5-20 best matches (~500ms)\n    ‚Üì\nFinal Results (Highly Relevant)\n```\n\n### When to Use Reranking\n\n**Use Reranking When**:\n- Precision is critical (e.g., customer support, legal search)\n- Retrieving small final result set (top 5-10)\n- Have budget for reranking API calls ($1-2 per 1000 queries)\n- Latency budget allows ~500ms for reranking\n\n**Skip Reranking When**:\n- Need sub-100ms response times\n- Large result sets (50+ results)\n- Cost-sensitive application\n- Hybrid search already provides good enough results\n\n## üèóÔ∏è Architecture\n\nThis notebook builds on Week 2 / Video 5 (Hybrid Search):\n\n1. **Previous**: Hybrid search with RRF fusion (dense + sparse)\n2. **New**: Add Cohere reranking as optional refinement step\n3. **Next**: Integrate reranking into FastAPI RAG pipeline\n\n## üìä Performance Characteristics\n\n| Stage | Latency | Cost | Accuracy |\n|-------|---------|------|----------|\n| Hybrid Search (top-20) | ~100ms | ~$0.0002/query | Good |\n| Reranking (top-5) | ~500ms | ~$0.002/query | Excellent |\n| **Total** | **~600ms** | **~$0.0022/query** | **Excellent** |\n\n**Cost Analysis** (1000 queries/day):\n- OpenAI embeddings: $0.20/month\n- Cohere reranking: $60/month (1K queries √ó $0.002 √ó 30 days)\n- **Total: ~$60/month** (reranking dominates cost)\n\n## üîß Setup Requirements\n\n- Qdrant running at `http://localhost:6333`\n- Collection: `Amazon-items-collection-01-hybrid-search` (from Video 5)\n- Environment variables:\n  - `OPENAI_API_KEY` - For embedding generation\n  - `COHERE_API_KEY` - For reranking (https://dashboard.cohere.com/api-keys)\n\n---\n\n## Import",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e441bdfe",
   "metadata": {},
   "outputs": [],
   "source": "# Vector database client and models for hybrid search\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import (\n    VectorParams, Distance, PayloadSchemaType, PointStruct,  # Collection configuration\n    SparseVectorParams, Document,  # Sparse vector (BM25) support\n    Prefetch, FusionQuery  # Hybrid search with RRF fusion\n)\nfrom qdrant_client import models\n\n# Data manipulation\nimport pandas as pd\n\n# LLM providers\nimport openai  # For embeddings (text-embedding-3-small)\nimport cohere  # For reranking (rerank-v4.0-pro)\n\n# Environment management\nimport os\nfrom dotenv import load_dotenv\n\n# Load API keys from .env file\nload_dotenv()"
  },
  {
   "cell_type": "markdown",
   "id": "4c5feaf2",
   "metadata": {},
   "source": "---\n\n## Stage 1: Hybrid Search Retrieval\n\nThis section implements the **initial retrieval** stage using hybrid search (from Week 2 / Video 5).\n\n### What This Does:\n1. Connect to Qdrant vector database\n2. Generate query embeddings using OpenAI text-embedding-3-small\n3. Perform hybrid search combining:\n   - **Dense vectors**: Semantic similarity (cosine distance)\n   - **Sparse vectors**: Keyword matching (BM25 algorithm)\n4. Fuse results using RRF (Reciprocal Rank Fusion)\n5. Return top-K candidate documents\n\n### Key Parameters:\n- `k=20`: Retrieve 20 candidates (more than final 5 to give reranker options)\n- `prefetch limit=20`: Each search method (dense + sparse) gets 20 candidates\n- Collection: `Amazon-items-collection-01-hybrid-search`\n\n### Performance:\n- **Latency**: ~100ms (fast enough for initial retrieval)\n- **Recall**: ~90% (hybrid search finds most relevant products)\n- **Precision**: ~70% (some irrelevant results, will be filtered by reranker)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb68a65",
   "metadata": {},
   "outputs": [],
   "source": "# Connect to Qdrant vector database running in Docker\n# URL: http://localhost:6333 (exposed by docker-compose.yml)\nqdrant_client = QdrantClient(url=\"http://localhost:6333\")\n\n\ndef get_embedding(text, model=\"text-embedding-3-small\"):\n    \"\"\"\n    Generate dense vector embedding using OpenAI's embedding model.\n    \n    Args:\n        text: String to embed (query or document text)\n        model: OpenAI embedding model (default: text-embedding-3-small)\n    \n    Returns:\n        List[float]: Dense vector of length 1536\n    \n    Performance:\n        - Latency: ~50-100ms per request\n        - Cost: $0.020 / 1M tokens (~$0.0002 per query)\n    \n    Why text-embedding-3-small:\n        - Good balance of quality and speed\n        - 1536 dimensions (smaller than -3-large's 3072)\n        - Sufficient for product search use case\n    \"\"\"\n    response = openai.embeddings.create(\n        input=[text],\n        model=model,\n    )\n    return response.data[0].embedding"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kt4atlstuc",
   "metadata": {},
   "outputs": [],
   "source": "def retrieve_data(query, qdrant_client, k=5):\n    \"\"\"\n    Perform hybrid search retrieval combining dense and sparse vectors.\n    \n    This is Stage 1 of the two-stage retrieval pipeline:\n    1. Fast hybrid search retrieves broad candidate set\n    2. (Next stage) Reranker refines results for precision\n    \n    Args:\n        query: User query string (e.g., \"Can I get a laptop?\")\n        qdrant_client: QdrantClient instance\n        k: Number of results to return (default=5, using 20 for reranking)\n    \n    Returns:\n        dict: {\n            \"retrieved_context_ids\": List of product ASINs\n            \"retrieved_context\": List of product descriptions\n            \"retrieved_context_ratings\": List of average ratings\n            \"similarity_scores\": List of RRF fusion scores\n        }\n    \n    Hybrid Search Strategy:\n        - Prefetch 20 candidates from EACH method (dense + sparse)\n        - Fuse using RRF (Reciprocal Rank Fusion)\n        - Return top-k after fusion\n    \n    Why k=20 for reranking:\n        - Reranker needs options to reorder (5 is too few)\n        - 20 is sweet spot: diverse enough, fast enough\n        - Final output will be top-5 after reranking\n    \"\"\"\n    \n    # Step 1: Generate dense query embedding (OpenAI API call ~50-100ms)\n    query_embedding = get_embedding(query)\n    \n    # Step 2: Perform hybrid search with prefetch + fusion\n    results = qdrant_client.query_points(\n        collection_name=\"Amazon-items-collection-01-hybrid-search\",\n        \n        # Prefetch: Retrieve candidates from EACH search method independently\n        prefetch=[\n            # Dense vector search (semantic similarity)\n            Prefetch(\n                query=query_embedding,  # 1536-dim vector from OpenAI\n                using=\"text-embedding-3-small\",  # Named vector in collection\n                limit=20  # Get top 20 from semantic search\n            ),\n            # Sparse vector search (BM25 keyword matching)\n            Prefetch(\n                query=Document(\n                    text=query,  # Raw query text (not embedded)\n                    model=\"qdrant/bm25\"  # Qdrant auto-computes BM25 vector\n                ),\n                using=\"bm25\",  # Named sparse vector in collection\n                limit=20  # Get top 20 from keyword search\n            )\n        ],\n        \n        # Fusion: Combine prefetch results using RRF (Reciprocal Rank Fusion)\n        # RRF formula: score = Œ£(1 / (k + rank_i)) where k=60\n        # Benefits: Scale-independent, no manual normalization needed\n        query=FusionQuery(fusion=\"rrf\"),\n        \n        # Final limit: Return top-k after fusion\n        limit=k,\n    )\n    \n    # Step 3: Extract results into structured format\n    retrieved_context_ids = []\n    retrieved_context = []\n    similarity_scores = []\n    retrieved_context_ratings = []\n    \n    for result in results.points:\n        # Product ID (Amazon ASIN)\n        retrieved_context_ids.append(result.payload[\"parent_asin\"])\n        \n        # Product description (will be reranked)\n        retrieved_context.append(result.payload[\"description\"])\n        \n        # Product rating (for context)\n        retrieved_context_ratings.append(result.payload[\"average_rating\"])\n        \n        # RRF fusion score (0-1 range, higher = more relevant)\n        similarity_scores.append(result.score)\n    \n    # Return structured dictionary\n    return {\n        \"retrieved_context_ids\": retrieved_context_ids,\n        \"retrieved_context\": retrieved_context,\n        \"retrieved_context_ratings\": retrieved_context_ratings,\n        \"similarity_scores\": similarity_scores,\n    }"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69025ee",
   "metadata": {},
   "outputs": [],
   "source": "# Test query: Simple natural language question about laptops\nquery = \"Can I get a laptop?\""
  },
  {
   "cell_type": "code",
   "id": "om3j0sqdeup",
   "source": "### Test Query\n\nLet's test with a laptop query to retrieve 20 candidates for reranking.\n\n**Why k=20?**\n- Too few (k=5): Reranker has limited options, can't improve much\n- Too many (k=50): Slower reranking, more API cost, diminishing returns\n- Sweet spot (k=20): Good diversity for reranker to optimize",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be94ffb",
   "metadata": {},
   "outputs": [],
   "source": "# Retrieve 20 candidates using hybrid search (Stage 1)\n# This gives the reranker a diverse set of products to reorder\nresults = retrieve_data(query, qdrant_client, k=20)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29aa0c5",
   "metadata": {},
   "outputs": [],
   "source": "# Display hybrid search results (Stage 1 output)\n# Note: These are ordered by RRF fusion score, but may not be perfectly relevant\n# Reranking will improve the ordering using cross-encoder model\nresults"
  },
  {
   "cell_type": "markdown",
   "id": "0be4c550",
   "metadata": {},
   "source": "---\n\n## Stage 2: Reranking with Cross-Encoder\n\nNow we use **Cohere's rerank model** to refine the candidate set with higher precision.\n\n### What Is Reranking?\n\n**Reranking** is the process of re-scoring and reordering an initial set of retrieved documents using a more powerful (but slower) model.\n\n### Cohere Rerank API\n\n**Model**: `rerank-v4.0-pro` (Cohere's latest production reranker)\n\n**How It Works**:\n1. Takes query + list of documents as input\n2. Encodes query and each document together (cross-encoder)\n3. Computes relevance score for each query-document pair\n4. Returns documents reordered by relevance score\n\n**Key Parameters**:\n- `model`: Which reranker to use (v4.0-pro is latest)\n- `query`: User query string\n- `documents`: List of candidate documents (from Stage 1)\n- `top_n`: How many results to return (can be less than input)\n\n### Cross-Encoder Architecture\n\n```\nInput: [Query, Document_1] ‚Üí Transformer ‚Üí Relevance Score: 0.92\nInput: [Query, Document_2] ‚Üí Transformer ‚Üí Relevance Score: 0.15\nInput: [Query, Document_3] ‚Üí Transformer ‚Üí Relevance Score: 0.78\n...\nOutput: Sorted by relevance [Doc_1, Doc_3, Doc_2, ...]\n```\n\n**Why More Accurate?**\n- Full attention between query and document tokens\n- Can identify subtle semantic relationships\n- No reliance on pre-computed vectors\n\n**Why Slower?**\n- Must run N forward passes (N = number of documents)\n- Can't pre-compute (query-dependent)\n- Latency: ~25ms per document (500ms for 20 docs)\n\n### Reranking vs Embedding Search\n\n| Aspect | Embedding Search (Bi-Encoder) | Reranking (Cross-Encoder) |\n|--------|-------------------------------|---------------------------|\n| **Speed** | Fast (~100ms for 1M docs) | Slow (~25ms per doc) |\n| **Accuracy** | Good (70-80% precision) | Excellent (90-95% precision) |\n| **Scalability** | Millions of docs | Hundreds of docs max |\n| **Pre-compute** | Yes (document embeddings) | No (query-dependent) |\n| **Use Case** | Initial retrieval | Final refinement |\n\n### Cost Analysis\n\n**Cohere Rerank Pricing** (as of 2024):\n- **rerank-v4.0-pro**: $2.00 per 1000 requests\n- Each request can rerank up to 100 documents\n- Typical usage: 20 documents per request\n\n**Example Costs**:\n- 1,000 queries √ó 20 docs = $2.00\n- 10,000 queries √ó 20 docs = $20.00\n- 100,000 queries √ó 20 docs = $200.00\n\n**Cost Comparison**:\n- Hybrid search only: ~$0.20 per 1K queries (OpenAI embeddings)\n- With reranking: ~$2.20 per 1K queries (10x more expensive)\n- **Trade-off**: Pay 10x for ~20% improvement in precision\n\n### When to Use Reranking\n\n‚úÖ **Use reranking when**:\n- Precision is critical (customer support, legal search)\n- Small final result set (top 5-10)\n- Have budget for API costs ($2/1000 queries)\n- Latency budget allows ~500ms\n\n‚ùå **Skip reranking when**:\n- Need sub-100ms response times\n- Large result sets (50+ results)\n- Cost-sensitive application\n- Hybrid search is good enough"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a78266",
   "metadata": {},
   "outputs": [],
   "source": "# Initialize Cohere client for reranking\n# Requires COHERE_API_KEY in environment (.env file)\n# Get your API key at: https://dashboard.cohere.com/api-keys\ncohere_client = cohere.ClientV2()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7a3af7",
   "metadata": {},
   "outputs": [],
   "source": "# Extract product descriptions from hybrid search results\n# These are the 20 candidates that will be reranked\n# Format: List of strings (product descriptions)\nto_rerank = results[\"retrieved_context\"]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddad2296",
   "metadata": {},
   "outputs": [],
   "source": "# Display the candidate documents (ordered by hybrid search RRF scores)\n# After reranking, these will be reordered by cross-encoder relevance scores\nto_rerank"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61648504",
   "metadata": {},
   "outputs": [],
   "source": "# Call Cohere Rerank API to reorder candidates by relevance\n# This is the core of Stage 2: cross-encoder reranking\nresponse = cohere_client.rerank(\n    # Model: rerank-v4.0-pro (latest production reranker)\n    # Alternatives: rerank-english-v3.0, rerank-multilingual-v3.0\n    model=\"rerank-v4.0-pro\",\n    \n    # Query: Same query used for hybrid search\n    query=query,\n    \n    # Documents: 20 candidates from Stage 1 (hybrid search)\n    # Format: List of strings (product descriptions)\n    documents=to_rerank,\n    \n    # Top N: Return all 20 (reordered by relevance)\n    # Could set to 5 to return only top 5, but we want to see full reordering\n    top_n=20,\n)\n\n# Response contains:\n#   - results: List of {index, relevance_score} sorted by score\n#   - index: Position in original to_rerank list\n#   - relevance_score: Float [0-1], higher = more relevant"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b666472",
   "metadata": {},
   "outputs": [],
   "source": "# Display raw reranking response\n# Shows: index (original position) and relevance_score for each document\n# Results are already sorted by relevance_score (descending)\nresponse"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f8c061",
   "metadata": {},
   "outputs": [],
   "source": "# Reconstruct reranked document list in new order\n# For each result, use its index to fetch the original document from to_rerank\n# Result: List of documents sorted by cross-encoder relevance (best first)\nreranked_results = [to_rerank[result.index] for result in response.results]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213eb892",
   "metadata": {},
   "outputs": [],
   "source": "# Display reranked results (Stage 2 output - final ranking)\n# Compare with original hybrid search results to see how ordering changed\n# Top results should now be more relevant to the query \"Can I get a laptop?\"\nreranked_results"
  },
  {
   "cell_type": "markdown",
   "id": "k5s8yg19min",
   "source": "---\n\n## üéì Key Takeaways\n\n### Two-Stage Retrieval Pipeline\n\n**Stage 1: Hybrid Search (Bi-Encoder)**\n- **Purpose**: Fast initial retrieval from large corpus\n- **Method**: Dense + sparse vectors with RRF fusion\n- **Output**: Top 20 candidates\n- **Latency**: ~100ms\n- **Accuracy**: Good recall (~90%), moderate precision (~70%)\n\n**Stage 2: Reranking (Cross-Encoder)**\n- **Purpose**: Refine ranking for final results\n- **Method**: Cohere rerank-v4.0-pro cross-encoder\n- **Output**: Top 20 reordered by relevance\n- **Latency**: ~500ms (25ms per document)\n- **Accuracy**: Excellent precision (~95%)\n\n### Comparison of Approaches\n\n| Approach | Latency | Cost/1K Queries | Precision | Best For |\n|----------|---------|----------------|-----------|----------|\n| **Dense only** | 50ms | $0.20 | 60% | High volume, cost-sensitive |\n| **Hybrid (Dense+Sparse)** | 100ms | $0.20 | 70% | General purpose, good balance |\n| **Hybrid + Rerank** | 600ms | $2.20 | 95% | High precision, low volume |\n\n### When Each Stage Matters\n\n**Skip Stage 1 (Hybrid Search)**:\n- ‚ùå Never skip Stage 1\n- Stage 1 is required for scalability (can't rerank millions of docs)\n\n**Skip Stage 2 (Reranking)**:\n- ‚úÖ Yes, if latency <200ms required\n- ‚úÖ Yes, if cost budget <$0.50 per 1K queries\n- ‚úÖ Yes, if hybrid search precision is sufficient\n- ‚ùå No, if precision is critical (support, legal, medical)\n\n### Integration with RAG Pipeline\n\n**Current Workflow**:\n```python\n# Stage 1: Hybrid search\ncandidates = retrieve_data(query, k=20)\n\n# Stage 2: Rerank\nreranked = cohere_client.rerank(\n    query=query,\n    documents=candidates[\"retrieved_context\"],\n    top_n=5\n)\n\n# Stage 3: LLM generation (not shown in this notebook)\ncontext = [candidates[\"retrieved_context\"][r.index] for r in reranked.results]\nanswer = llm.generate(query=query, context=context)\n```\n\n**Next Steps**:\n1. Add reranking to FastAPI RAG endpoint (optional flag)\n2. A/B test reranked vs non-reranked results\n3. Measure impact on RAGAS metrics (faithfulness, relevance)\n4. Monitor latency and cost in production\n\n### Cost-Benefit Analysis\n\n**Scenario: 10,000 queries/month**\n\n| Approach | Total Cost | Latency | Precision |\n|----------|-----------|---------|-----------|\n| Hybrid only | $2 | 100ms | 70% |\n| Hybrid + Rerank | $22 | 600ms | 95% |\n\n**Is it worth it?**\n- Extra cost: $20/month ($0.002 per query)\n- Extra latency: 500ms (6x slower)\n- Precision gain: +25% (70% ‚Üí 95%)\n- **Decision**: Depends on use case value and budget\n\n### Production Considerations\n\n**Latency Optimization**:\n1. **Async reranking**: Don't block on rerank if not critical\n2. **Batch requests**: Rerank multiple queries together\n3. **Cache results**: Cache reranked results for popular queries\n4. **Selective reranking**: Only rerank queries that need it (e.g., low confidence)\n\n**Cost Optimization**:\n1. **Reduce top_n**: Rerank top 10 instead of top 20 (50% cost savings)\n2. **Hybrid-first**: Try hybrid search first, only rerank if confidence is low\n3. **Free alternatives**: Self-host reranker (e.g., bge-reranker-v2-m3)\n4. **Caching**: Cache reranked results for repeated queries\n\n**Quality Monitoring**:\n1. Track reranking impact on metrics (RAGAS, user feedback)\n2. Compare reranked vs non-reranked results\n3. A/B test with real users\n4. Monitor for model drift (reranker quality over time)\n\n### Alternative Reranking Models\n\n**Cohere Rerank**:\n- ‚úÖ Best accuracy (state-of-the-art)\n- ‚úÖ Multilingual support\n- ‚úÖ Easy API integration\n- ‚ùå Most expensive ($2/1K requests)\n\n**Self-Hosted (bge-reranker-v2-m3)**:\n- ‚úÖ Free (after infrastructure costs)\n- ‚úÖ Full control, no rate limits\n- ‚úÖ Privacy (data stays on-prem)\n- ‚ùå Requires GPU inference server\n- ‚ùå Need to manage scaling and updates\n\n**LLM as Reranker (GPT-4)**:\n- ‚úÖ Can provide explanations\n- ‚úÖ Can follow custom ranking criteria\n- ‚ùå Very slow (~2s per query)\n- ‚ùå Expensive (~$0.10 per query)\n- ‚ùå Not designed for reranking\n\n### Further Learning\n\n**Topics to Explore**:\n1. **Listwise reranking**: Rerank all docs simultaneously (vs pairwise)\n2. **Learning to rank**: Train custom reranker on your data\n3. **Multi-stage retrieval**: Add more stages (e.g., Stage 3: LLM reranking)\n4. **Query classification**: Decide when to use reranking dynamically\n\n**Resources**:\n- Cohere Rerank Docs: https://docs.cohere.com/docs/reranking\n- BEIR Benchmark: https://github.com/beir-cellar/beir (reranking leaderboard)\n- Sentence Transformers: https://www.sbert.net/examples/applications/cross-encoder/README.html\n\n---\n\n## ‚úÖ Summary\n\nYou've learned:\n1. ‚úÖ **Why reranking matters**: Cross-encoders are more accurate than bi-encoders\n2. ‚úÖ **Two-stage retrieval**: Fast hybrid search ‚Üí slow cross-encoder refinement\n3. ‚úÖ **Cohere Rerank API**: How to use rerank-v4.0-pro model\n4. ‚úÖ **Trade-offs**: Latency (+500ms), cost (+10x), precision (+25%)\n5. ‚úÖ **Integration pattern**: How to add reranking to existing RAG pipeline\n\n**Next**: Integrate reranking into FastAPI backend and measure impact on RAG quality.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}