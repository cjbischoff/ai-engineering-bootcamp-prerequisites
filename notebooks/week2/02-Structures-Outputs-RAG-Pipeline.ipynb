{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "518317b8",
   "metadata": {},
   "source": [
    "# Week 2 Video 2: Structured Outputs with Grounding Context\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates **grounding** in RAG systems - the practice of linking generated answers back to their source documents. This is crucial for:\n",
    "\n",
    "- **Transparency**: Users can see which products influenced the answer\n",
    "- **Verification**: Users can validate the AI's reasoning\n",
    "- **Trust**: Providing sources increases confidence in recommendations\n",
    "- **Debugging**: Developers can trace why certain products were chosen\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### What is Grounding?\n",
    "\n",
    "Grounding means returning not just the answer, but also **references** to the source documents used. In our case:\n",
    "- Product IDs (ASINs) that were used\n",
    "- Short descriptions of those products\n",
    "- This allows users to \"click through\" to see the original products\n",
    "\n",
    "### Building on Video 1\n",
    "\n",
    "Video 1 showed basic structured outputs with `RAGGenerationResponse(answer: str)`. Now we extend this to include a `references` field containing a list of `RAGUsedContext` objects.\n",
    "\n",
    "### The Challenge\n",
    "\n",
    "The LLM must:\n",
    "1. Answer the question\n",
    "2. Identify which products it actually used\n",
    "3. Provide concise descriptions of those products\n",
    "\n",
    "This requires careful prompt engineering to guide the model.\n",
    "\n",
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822a7788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import instructor\n",
    "from qdrant_client import QdrantClient\n",
    "from pydantic import BaseModel, Field\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece25ba8",
   "metadata": {},
   "source": [
    "# Part 1: Baseline RAG Pipeline (From Video 1)\n",
    "\n",
    "This section demonstrates the basic RAG pipeline from Video 1 using structured outputs.\n",
    "\n",
    "**Key Features:**\n",
    "- Simple `RAGGenerationResponse` with just an `answer` field\n",
    "- Basic prompt without grounding requirements\n",
    "- Returns the answer as a validated Pydantic model\n",
    "\n",
    "**What's Missing:**\n",
    "- No references to source products\n",
    "- Can't verify which products influenced the answer\n",
    "- Users must trust the AI without being able to check sources\n",
    "\n",
    "This baseline helps us understand what we're improving in Part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74248fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = instructor.from_openai(openai.OpenAI())\n",
    "\n",
    "class RAGGenerationResponse(BaseModel):\n",
    "    answer: str = Field(description=\"The answer to the question\")\n",
    "\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    Convert text to 1536-dimensional embedding vector using OpenAI.\n",
    "\n",
    "    Args:\n",
    "        text: Input text to embed (query or document)\n",
    "        model: OpenAI embedding model name\n",
    "\n",
    "    Returns:\n",
    "        List of floats representing the embedding vector\n",
    "    \"\"\"\n",
    "    response = openai.embeddings.create(\n",
    "        input=text,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    return response.data[0].embedding\n",
    "\n",
    "\n",
    "def retrieve_data(query, qdrant_client, k=5):\n",
    "    \"\"\"\n",
    "    Retrieve top-k most relevant products from Qdrant vector database.\n",
    "\n",
    "    Args:\n",
    "        query: User's question text\n",
    "        qdrant_client: Connected Qdrant client instance\n",
    "        k: Number of similar products to retrieve\n",
    "\n",
    "    Returns:\n",
    "        Dict with retrieved_context_ids, retrieved_context,\n",
    "        retrieved_context_ratings, and similarity_scores\n",
    "    \"\"\"\n",
    "    # Convert query to embedding for semantic search\n",
    "    query_embedding = get_embedding(query)\n",
    "\n",
    "    # Search Qdrant for nearest neighbors using cosine similarity\n",
    "    results = qdrant_client.query_points(\n",
    "        collection_name=\"Amazon-items-collection-00\",\n",
    "        query=query_embedding,\n",
    "        limit=k,\n",
    "    )\n",
    "\n",
    "    # Extract metadata from matching results\n",
    "    retrieved_context_ids = []\n",
    "    retrieved_context = []\n",
    "    similarity_scores = []\n",
    "    retrieved_context_ratings = []\n",
    "\n",
    "    for result in results.points:\n",
    "        retrieved_context_ids.append(result.payload[\"parent_asin\"])\n",
    "        retrieved_context.append(result.payload[\"description\"])\n",
    "        retrieved_context_ratings.append(result.payload[\"average_rating\"])\n",
    "        similarity_scores.append(result.score)\n",
    "\n",
    "    return {\n",
    "        \"retrieved_context_ids\": retrieved_context_ids,\n",
    "        \"retrieved_context\": retrieved_context,\n",
    "        \"retrieved_context_ratings\": retrieved_context_ratings,\n",
    "        \"similarity_scores\": similarity_scores,\n",
    "    }\n",
    "\n",
    "\n",
    "def process_context(context):\n",
    "    \"\"\"\n",
    "    Format retrieved product data into readable text for LLM.\n",
    "\n",
    "    Args:\n",
    "        context: Dict with retrieved_context_ids, retrieved_context,\n",
    "                and retrieved_context_ratings\n",
    "\n",
    "    Returns:\n",
    "        Formatted string with product information\n",
    "    \"\"\"\n",
    "    formatted_context = \"\"\n",
    "\n",
    "    for id, chunk, rating in zip(\n",
    "        context[\"retrieved_context_ids\"],\n",
    "        context[\"retrieved_context\"],\n",
    "        context[\"retrieved_context_ratings\"],\n",
    "    ):\n",
    "        formatted_context += f\"- ID: {id}, rating: {rating}, description: {chunk}\\n\"\n",
    "\n",
    "    return formatted_context\n",
    "\n",
    "\n",
    "def build_prompt(preprocessed_context, question):\n",
    "    \"\"\"\n",
    "    Construct the final prompt sent to the language model.\n",
    "\n",
    "    Args:\n",
    "        preprocessed_context: Formatted string of retrieved products\n",
    "        question: User's original question\n",
    "\n",
    "    Returns:\n",
    "        Complete prompt with system instructions, context, and question\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are a shopping assistant that can answer questions about the products in stock.\n",
    "\n",
    "You will be given a question and a list of context.\n",
    "\n",
    "Instructions:\n",
    "- You need to answer the question based on the provided context only.\n",
    "- Never use word context and refer to it as the available products.\n",
    "\n",
    "Context:\n",
    "{preprocessed_context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate_answer(prompt):\n",
    "    \"\"\"\n",
    "    Generate structured answer using instructor library for type-safe responses.\n",
    "\n",
    "    **KEY CHANGE FROM WEEK 1:** Now returns structured Pydantic model instead of raw string.\n",
    "\n",
    "    Args:\n",
    "        prompt: Complete prompt with instructions, context, and question\n",
    "\n",
    "    Returns:\n",
    "        RAGGenerationResponse Pydantic model with validated answer field\n",
    "\n",
    "    Note:\n",
    "        - Must use instructor client, NOT raw openai client\n",
    "        - create_with_completion() returns (model, raw_response) tuple\n",
    "        - Response IS the Pydantic model, not nested in .choices array\n",
    "    \"\"\"\n",
    "    # CRITICAL: Create instructor client for structured outputs\n",
    "    # This patches the OpenAI client to support response_model parameter\n",
    "    instructor_client = instructor.from_openai(openai.OpenAI())\n",
    "\n",
    "    # Use instructor's create_with_completion for structured output\n",
    "    # Returns both the validated Pydantic model and raw API response\n",
    "    response, raw_response = instructor_client.chat.completions.create_with_completion(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "        temperature=0,\n",
    "        response_model=RAGGenerationResponse,  # Pydantic model enforces structure\n",
    "    )\n",
    "\n",
    "    # response is already a RAGGenerationResponse object - no need to extract from .choices\n",
    "    return response\n",
    "\n",
    "\n",
    "def rag_pipeline(question, top_k=5):\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline with structured outputs.\n",
    "\n",
    "    **ENHANCED IN WEEK 2:** Now returns structured Pydantic models instead of raw text.\n",
    "\n",
    "    Args:\n",
    "        question: User's question about products\n",
    "        top_k: Number of products to retrieve (default: 5)\n",
    "\n",
    "    Returns:\n",
    "        Dict with:\n",
    "            - datamodel: RAGGenerationResponse Pydantic model\n",
    "            - answer: Extracted answer string (for convenience)\n",
    "            - question: Original user query\n",
    "            - retrieved_context_ids: Product ASINs used\n",
    "            - retrieved_context: Product descriptions used\n",
    "            - similarity_scores: Relevance scores for each product\n",
    "    \"\"\"\n",
    "    # Initialize Qdrant client\n",
    "    qdrant_client = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "    # Step 1: Retrieve relevant products from vector database\n",
    "    retrieved_context = retrieve_data(question, qdrant_client, top_k)\n",
    "\n",
    "    # Step 2: Format products into readable text\n",
    "    preprocessed_context = process_context(retrieved_context)\n",
    "\n",
    "    # Step 3: Build complete prompt with instructions and context\n",
    "    prompt = build_prompt(preprocessed_context, question)\n",
    "\n",
    "    # Step 4: Generate structured answer using instructor (NEW IN WEEK 2)\n",
    "    answer = generate_answer(prompt)\n",
    "\n",
    "    # Step 5: Package result with both structured model and metadata\n",
    "    final_result = {\n",
    "        \"datamodel\": answer,  # Full Pydantic model\n",
    "        \"answer\": answer.answer,  # Extracted answer string for convenience\n",
    "        \"question\": question,\n",
    "        \"retrieved_context_ids\": retrieved_context[\"retrieved_context_ids\"],\n",
    "        \"retrieved_context\": retrieved_context[\"retrieved_context\"],\n",
    "        \"similarity_scores\": retrieved_context[\"similarity_scores\"],\n",
    "    }\n",
    "\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4b9a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_client = QdrantClient(url=\"http://localhost:6333\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97ba58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG: Don't pass qdrant_client - function creates its own internally\n",
    "# output = rag_pipeline(\n",
    "#     \"Can I get a charging cable? Please suggest me a good one.\", qdrant_client\n",
    "# )\n",
    "\n",
    "# CORRECT: Only pass the question (and optionally top_k)\n",
    "output = rag_pipeline(\n",
    "    \"Can I get a charging cable? Please suggest me a good one.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eef099",
   "metadata": {},
   "outputs": [],
   "source": [
    "output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ae3717",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816ddb70",
   "metadata": {},
   "source": [
    "# Part 2: RAG Pipeline with Grounding Context\n",
    "\n",
    "## What's New in This Section\n",
    "\n",
    "This section extends the RAG pipeline to include **grounding** - returning references to the source products used in the answer.\n",
    "\n",
    "### Nested Pydantic Models\n",
    "\n",
    "We now have **two** Pydantic models working together:\n",
    "\n",
    "1. **`RAGUsedContext`**: Represents a single product reference\n",
    "   - `id`: The product ASIN (Amazon Standard Identification Number)\n",
    "   - `description`: A concise 1-2 sentence summary of the product\n",
    "\n",
    "2. **`RAGGenerationResponse`**: The complete structured output\n",
    "   - `answer`: The full answer to the user's question\n",
    "   - `references`: A list of `RAGUsedContext` objects\n",
    "\n",
    "### Why Nested Models?\n",
    "\n",
    "Nested models allow us to represent **complex, structured data**:\n",
    "- The outer model (`RAGGenerationResponse`) represents the complete response\n",
    "- The inner model (`RAGUsedContext`) represents each individual reference\n",
    "- instructor validates the entire nested structure automatically\n",
    "\n",
    "### Enhanced Prompt Engineering\n",
    "\n",
    "The prompt in `build_prompt()` now explicitly instructs the LLM to:\n",
    "- Identify which products were actually used in the answer\n",
    "- Only include products that contributed to the response (not all retrieved products)\n",
    "- Provide concise descriptions with product names\n",
    "- Format the answer with detailed specifications in bullet points\n",
    "\n",
    "This is a key insight: **Structured outputs require structured prompts**. The more specific your prompt, the better the structured output.\n",
    "\n",
    "### Key Differences from Part 1\n",
    "\n",
    "| Aspect | Part 1 (Baseline) | Part 2 (With Grounding) |\n",
    "|--------|-------------------|-------------------------|\n",
    "| Pydantic Model | Simple (1 field) | Nested (2 models) |\n",
    "| Prompt | Basic instructions | Explicit grounding requirements |\n",
    "| Return Value | Just answer string | Answer + references list |\n",
    "| Transparency | None | Full source attribution |\n",
    "| Use Case | Quick answers | Production systems requiring verification |\n",
    "\n",
    "## Define Pydantic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3407dd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define nested Pydantic models for grounded structured outputs\n",
    "\n",
    "class RAGUsedContext(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents a single product reference (grounding context).\n",
    "    \n",
    "    This model captures information about one product that was used\n",
    "    to generate the answer. Multiple instances of this model will be\n",
    "    returned in the references list.\n",
    "    \n",
    "    Fields:\n",
    "        id: The product ASIN (Amazon Standard Identification Number)\n",
    "            - Unique identifier for the product\n",
    "            - Can be used to link back to full product details\n",
    "            - Example: \"B09X12ABC\"\n",
    "        \n",
    "        description: Short human-readable summary (1-2 sentences)\n",
    "            - Should include the product name\n",
    "            - Brief description of what the product is\n",
    "            - Example: \"GREPHONE USB-C to Lightning Cable - 6ft MFi certified charging cable\"\n",
    "    \n",
    "    Why this structure?\n",
    "        - Balances detail vs. brevity\n",
    "        - ID enables programmatic lookup\n",
    "        - Description enables human understanding\n",
    "    \"\"\"\n",
    "    id: str = Field(description=\"The unique identifier to the answer the question\")\n",
    "    description: str = Field(description=\"Short description of the item used to answer the question\")\n",
    "\n",
    "\n",
    "class RAGGenerationResponse(BaseModel):\n",
    "    \"\"\"\n",
    "    Complete structured response with grounding context.\n",
    "    \n",
    "    This is the TOP-LEVEL model that instructor will validate and return.\n",
    "    It contains both the answer and the list of source references.\n",
    "    \n",
    "    Fields:\n",
    "        answer: The full natural language answer to the user's question\n",
    "            - Should be detailed and informative\n",
    "            - Should reference products by name\n",
    "            - Should include specifications in bullet points\n",
    "        \n",
    "        references: List of RAGUsedContext objects (grounding)\n",
    "            - CRITICAL: Only include products actually used in the answer\n",
    "            - Not all retrieved products will be used\n",
    "            - The LLM must decide which products are relevant\n",
    "            - Each reference links the answer back to its source\n",
    "    \n",
    "    Why nested structure?\n",
    "        - Instructor automatically validates nested Pydantic models\n",
    "        - Type-safe: references must be list[RAGUsedContext]\n",
    "        - Self-documenting: structure makes relationship clear\n",
    "        - Enables rich tooling: IDEs can autocomplete fields\n",
    "    \n",
    "    Common Pitfall:\n",
    "        Don't confuse retrieved_context (all products fetched from DB)\n",
    "        with references (only products used in answer). The LLM performs\n",
    "        this filtering based on relevance to the specific question.\n",
    "    \"\"\"\n",
    "    answer: str = Field(description=\"The answer to the question\")\n",
    "    references: list[RAGUsedContext] = Field(description=\"List of items used to generate the answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b329676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== RETRIEVAL FUNCTIONS =====\n",
    "# These functions are the same as Part 1 - grounding happens at the prompt/model level\n",
    "\n",
    "def retrieve_data(query, qdrant_client, k=5):\n",
    "    \"\"\"\n",
    "    Retrieve top-k most relevant products from vector database.\n",
    "    \n",
    "    This function is UNCHANGED from Part 1. Grounding doesn't change retrieval,\n",
    "    it changes what we do with the retrieved data.\n",
    "    \n",
    "    Args:\n",
    "        query: User's question text\n",
    "        qdrant_client: Connected Qdrant instance\n",
    "        k: Number of products to retrieve from vector DB\n",
    "    \n",
    "    Returns:\n",
    "        Dict with lists of IDs, descriptions, ratings, and similarity scores\n",
    "    \"\"\"\n",
    "    # Convert text query to 1536-dim embedding vector (same semantic space as stored products)\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    # Search Qdrant using cosine similarity to find k nearest neighbors\n",
    "    results = qdrant_client.query_points(\n",
    "        collection_name=\"Amazon-items-collection-00\",  # Collection from Week 1 preprocessing\n",
    "        query=query_embedding,\n",
    "        limit=k,  # Retrieve top-k most similar products\n",
    "    )\n",
    "    \n",
    "    # Initialize lists to store extracted product metadata\n",
    "    retrieved_context_ids = []\n",
    "    retrieved_context = []\n",
    "    similarity_scores = []\n",
    "    retrieved_context_ratings = []\n",
    "    \n",
    "    # Extract metadata from each result point\n",
    "    for result in results.points:\n",
    "        retrieved_context_ids.append(result.payload[\"parent_asin\"])  # Product ID\n",
    "        retrieved_context.append(result.payload[\"description\"])  # Full description\n",
    "        retrieved_context_ratings.append(result.payload[\"average_rating\"])  # Customer rating\n",
    "        similarity_scores.append(result.score)  # Cosine similarity (0-1, higher = more similar)\n",
    "    \n",
    "    # Return structured dict (NOT a Pydantic model - this is internal data structure)\n",
    "    return {\n",
    "        \"retrieved_context_ids\": retrieved_context_ids,\n",
    "        \"retrieved_context\": retrieved_context,\n",
    "        \"retrieved_context_ratings\": retrieved_context_ratings,\n",
    "        \"similarity_scores\": similarity_scores,\n",
    "    }\n",
    "\n",
    "\n",
    "def process_context(context):\n",
    "    \"\"\"\n",
    "    Format retrieved products into text for the LLM prompt.\n",
    "    \n",
    "    This function is UNCHANGED from Part 1. It converts structured data\n",
    "    into a formatted string that the LLM can parse.\n",
    "    \n",
    "    Args:\n",
    "        context: Dict from retrieve_data() with product information\n",
    "    \n",
    "    Returns:\n",
    "        Multi-line string with one product per line\n",
    "        Example: \"- ID: B09X12ABC, rating: 4.5, description: USB cable...\"\n",
    "    \"\"\"\n",
    "    formatted_context = \"\"\n",
    "    \n",
    "    # Build formatted string with bullet points for each product\n",
    "    # zip() iterates through three lists in parallel\n",
    "    for id, chunk, rating in zip(\n",
    "        context[\"retrieved_context_ids\"],\n",
    "        context[\"retrieved_context\"],\n",
    "        context[\"retrieved_context_ratings\"]\n",
    "    ):\n",
    "        formatted_context += f\"- ID: {id}, rating: {rating}, description: {chunk}\\n\"\n",
    "    \n",
    "    return formatted_context\n",
    "\n",
    "\n",
    "# ===== PROMPT ENGINEERING FOR GROUNDING =====\n",
    "# This is where the magic happens - the prompt tells the LLM to return grounded references\n",
    "\n",
    "def build_prompt(preprocessed_context, question):\n",
    "    \"\"\"\n",
    "    Build enhanced prompt that explicitly requests grounding context.\n",
    "    \n",
    "    **KEY CHANGE FROM PART 1**: This prompt now instructs the LLM to:\n",
    "    1. Identify which products were ACTUALLY used (not all retrieved products)\n",
    "    2. Return product IDs for those products\n",
    "    3. Provide concise descriptions\n",
    "    \n",
    "    This is a critical insight: Structured outputs require structured prompts.\n",
    "    The Pydantic model defines the STRUCTURE, but the prompt guides the CONTENT.\n",
    "    \n",
    "    Args:\n",
    "        preprocessed_context: Formatted string with all retrieved products\n",
    "        question: User's original question\n",
    "    \n",
    "    Returns:\n",
    "        Complete prompt with instructions for grounded structured output\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are a shopping assistant that can answer questions about the products in stock.\n",
    "\n",
    "You will be given a question and a list of context.\n",
    "\n",
    "Instructions:\n",
    "- You need to answer the question based on the provided context only.\n",
    "- Never use word context and refer to it as the available products.\n",
    "- As an output you need to provide:\n",
    "\n",
    "* The answer to the question based on the provided context.\n",
    "* The list of the IDs of the chunks that were used to answer the question. Only return the ones that are used in the answer.\n",
    "* Short description (1-2 sentences) of the item based on the description provided in the context.\n",
    "\n",
    "- The short description should have the name of the item.\n",
    "- The answer to the question should contain detailed information about the product and returned with detailed specification in bullet points.\n",
    "\n",
    "Context:\n",
    "{preprocessed_context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# ===== GENERATION WITH INSTRUCTOR =====\n",
    "# Instructor enforces the nested Pydantic structure\n",
    "\n",
    "def generate_answer(prompt):\n",
    "    \"\"\"\n",
    "    Generate STRUCTURED answer with grounding using instructor library.\n",
    "    \n",
    "    **UNCHANGED FROM PART 1** - instructor handles nested models automatically!\n",
    "    The only change is that RAGGenerationResponse now has a references field.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Complete prompt with grounding instructions\n",
    "    \n",
    "    Returns:\n",
    "        RAGGenerationResponse with answer AND references fields populated\n",
    "    \n",
    "    How instructor works:\n",
    "        1. Sends prompt to OpenAI with response_model=RAGGenerationResponse\n",
    "        2. OpenAI generates JSON matching the Pydantic schema\n",
    "        3. instructor validates the JSON against the schema\n",
    "        4. Returns a fully validated Pydantic object\n",
    "        5. Raises ValidationError if JSON doesn't match schema\n",
    "    \"\"\"\n",
    "    # Create instructor-patched client (enables response_model parameter)\n",
    "    instructor_client = instructor.from_openai(openai.OpenAI())\n",
    "    \n",
    "    # Generate structured output with automatic validation\n",
    "    # create_with_completion returns (validated_model, raw_api_response)\n",
    "    response, raw_response = instructor_client.chat.completions.create_with_completion(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "        temperature=0,  # Deterministic for consistency\n",
    "        response_model=RAGGenerationResponse,  # Nested Pydantic model (with references!)\n",
    "    )\n",
    "    \n",
    "    # response is already a RAGGenerationResponse object with validated fields:\n",
    "    # - response.answer (str)\n",
    "    # - response.references (list[RAGUsedContext])\n",
    "    return response\n",
    "\n",
    "\n",
    "# ===== ORCHESTRATION =====\n",
    "# The main pipeline function that ties everything together\n",
    "\n",
    "def rag_pipeline(question, top_k=5):\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline with GROUNDING CONTEXT.\n",
    "    \n",
    "    **ENHANCED FROM PART 1**: Now returns references field with source attribution.\n",
    "    \n",
    "    Pipeline Flow:\n",
    "        1. Retrieve top-k products from Qdrant (semantic search)\n",
    "        2. Format products into text prompt\n",
    "        3. Build enhanced prompt with grounding instructions\n",
    "        4. Generate structured answer with references using instructor\n",
    "        5. Package result with both answer and source attribution\n",
    "    \n",
    "    Args:\n",
    "        question: User's question about products\n",
    "        top_k: Number of products to retrieve (default: 5)\n",
    "    \n",
    "    Returns:\n",
    "        Dict with:\n",
    "            - original_output: Full RAGGenerationResponse Pydantic model\n",
    "            - answer: Extracted answer string (convenience field)\n",
    "            - references: Extracted references list (NEW IN VIDEO 2!)\n",
    "            - question: Echo back user query\n",
    "            - retrieved_context_ids: ALL products retrieved from DB\n",
    "            - retrieved_context: ALL product descriptions\n",
    "            - similarity_scores: Relevance scores for retrieved products\n",
    "    \n",
    "    Key Insight:\n",
    "        retrieved_context_ids (all 5-10 products) != references (only 2-3 used)\n",
    "        The LLM performs intelligent filtering based on relevance!\n",
    "    \n",
    "    COMMON ERROR:\n",
    "        DO NOT pass qdrant_client as an argument!\n",
    "        Function signature: rag_pipeline(question, top_k=5)\n",
    "        The function creates its own QdrantClient internally.\n",
    "        \n",
    "        WRONG: rag_pipeline(\"question\", qdrant_client, top_k=10)\n",
    "        RIGHT: rag_pipeline(\"question\", top_k=10)\n",
    "    \"\"\"\n",
    "    # Initialize Qdrant client (connects to local instance on port 6333)\n",
    "    qdrant_client = QdrantClient(url=\"http://localhost:6333\")\n",
    "    \n",
    "    # Step 1: Retrieve top-k products from vector database\n",
    "    retrieved_context = retrieve_data(question, qdrant_client, top_k)\n",
    "    \n",
    "    # Step 2: Format products into readable text for LLM\n",
    "    preprocessed_context = process_context(retrieved_context)\n",
    "    \n",
    "    # Step 3: Build prompt with grounding instructions\n",
    "    prompt = build_prompt(preprocessed_context, question)\n",
    "    \n",
    "    # Step 4: Generate structured answer WITH REFERENCES using instructor\n",
    "    answer = generate_answer(prompt)  # Returns RAGGenerationResponse with answer + references\n",
    "    \n",
    "    # Step 5: Package complete result\n",
    "    final_result = {\n",
    "        \"original_output\": answer,  # Full Pydantic model (access with .answer and .references)\n",
    "        \"answer\": answer.answer,  # Extracted answer string for convenience\n",
    "        \"references\": answer.references,  # NEW! List of RAGUsedContext objects\n",
    "        \"question\": question,  # Echo back for logging\n",
    "        \"retrieved_context_ids\": retrieved_context[\"retrieved_context_ids\"],  # All retrieved (for comparison)\n",
    "        \"retrieved_context\": retrieved_context[\"retrieved_context\"],  # All descriptions\n",
    "        \"similarity_scores\": retrieved_context[\"similarity_scores\"],  # Relevance scores\n",
    "    }\n",
    "    \n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dacd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG: Missing closing parenthesis and passing qdrant_client\n",
    "# result = rag_pipeline(\"Can I get some earphones?\", qdrant_client, top_k=10\n",
    "\n",
    "# CORRECT: Only pass question and top_k parameter\n",
    "result = rag_pipeline(\"Can I get some earphones?\", top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80429bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e93dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[\"answer\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7azriw9t1m6",
   "metadata": {},
   "source": [
    "## Accessing the Grounding Context\n",
    "\n",
    "The key benefit of grounding is **transparency**. Let's explore the structured output and see how references differ from retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mfc34sqjy4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the references field (NEW IN VIDEO 2!)\n",
    "# This contains ONLY the products that were actually used in the answer\n",
    "print(f\"Number of products USED in answer: {len(result['references'])}\")\n",
    "print(f\"Number of products RETRIEVED from DB: {len(result['retrieved_context_ids'])}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GROUNDING CONTEXT (Products actually used):\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "for i, ref in enumerate(result['references'], 1):\n",
    "    print(f\"{i}. Product ID: {ref.id}\")\n",
    "    print(f\"   Description: {ref.description}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "net1cr5an1s",
   "metadata": {},
   "source": [
    "### Key Insight: Retrieval vs. Usage\n",
    "\n",
    "Notice how the number of **retrieved** products (e.g., 10) is different from the number **used** in the answer (e.g., 2-3).\n",
    "\n",
    "**Why this matters:**\n",
    "- Not all retrieved products are relevant to every question\n",
    "- The LLM intelligently filters based on the specific question\n",
    "- Grounding shows which products actually influenced the answer\n",
    "- This enables verification: users can check if the answer is justified\n",
    "\n",
    "**Production Use Cases:**\n",
    "- E-commerce: \"Show me the products you based this recommendation on\"\n",
    "- Legal: \"Show me the case law citations\"\n",
    "- Medical: \"Show me the research papers supporting this advice\"\n",
    "- Customer support: \"Show me the KB articles referenced\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
