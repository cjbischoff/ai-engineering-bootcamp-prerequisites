{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "048891ea",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51124f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import json\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Filter, FieldCondition, MatchValue\n",
    "\n",
    "from langsmith import Client\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a96b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_client = QdrantClient(url=\"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb749cd",
   "metadata": {},
   "source": [
    "# Download all the data from QDrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9e76ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_points = qdrant_client.scroll(\n",
    "    collection_name=\"Amazon-items-collection-00\",\n",
    "    limit=100,\n",
    "    offset=None,\n",
    "    with_payload=True,\n",
    "    with_vectors=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3f71d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_points[0][0].payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5c2b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_context = [{\"id\": data.payload[\"parent_asin\"], \"text\": data.payload[\"description\"]} for data in all_points[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2869409d",
   "metadata": {},
   "source": [
    "# Render a prompt to generate synthetic Eval reference dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143e3f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "output_schema = {\n",
    "    \"type\": \"array\",\n",
    "    \"items\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"question\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Suggested question.\",\n",
    "            },\n",
    "            \"chunk_ids\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"ID of the chunk that could be used to answer the question.\",\n",
    "                },\n",
    "            },\n",
    "            \"answer_example\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Suggested answer grounded in the context.\",\n",
    "            },\n",
    "            \"reasoning\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Reasoning why the question could be answered with the chunks.\",\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "SYSTEM_PROMPT = f\"\"\"\n",
    "I am building a RAG application. I have a collection of 50 chunks of text.\n",
    "The RAG application will act as a shopping assistant that can answer questions about the stock of the products we have available.\n",
    "I will provide all of the available products to you with IDs of each chunk.\n",
    "I want you to come up with 30 questions to which the answers could be grounded in the chunk context.\n",
    "The questions should imitate a potential real user of this RAG system.\n",
    "As an output I need you to provide me the list of questions and the IDs of the chunks that could be used to answer them.\n",
    "Also, provide an example answer to the question given the context of the chunks.\n",
    "Also, provide the reason why you chose the chunks to answer the questions.\n",
    "Construct 10 that could use multiple chunks in the answer.\n",
    "Construct 15 questions that could use single chunk in the answer.\n",
    "Construct 5 questions that can't be answered with the available chunks.\n",
    "\n",
    "<OUTPUT JSON SCHEMA>\n",
    "{json.dumps(output_schema, indent=2)}\n",
    "</OUTPUT JSON SCHEMA>\n",
    "\n",
    "I need to be able to parse the json output.\n",
    "\"\"\"\n",
    "\n",
    "USER_PROMPT = f\"\"\"\n",
    "Here is the list of chunks, each list element is a dictionary with id and text:\n",
    "{all_context}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4qvpnohx7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SYSTEM_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z8kzp6oo15",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": USER_PROMPT}\n",
    "    ],\n",
    "    reasoning_effort=\"minimal\"\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e0014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_output = response.choices[0].message.content\n",
    "json_output = json.loads(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299377b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6b15c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4y4io9h7829",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = qdrant_client.scroll(\n",
    "    collection_name=\"Amazon-items-collection-00\",\n",
    "    scroll_filter=Filter(\n",
    "        must=[FieldCondition(key=\"parent_asin\", match=MatchValue(value=\"B0CBMPG524\"))]\n",
    "    ),\n",
    "    limit=100,\n",
    "    with_payload=True,\n",
    "    with_vectors=False,\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7c41bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "points[0].payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de3f5aggad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_description(parent_asin: str) -> str:\n",
    "    points = qdrant_client.scroll(\n",
    "        collection_name=\"Amazon-items-collection-00\",\n",
    "        scroll_filter=Filter(\n",
    "            must=[\n",
    "                FieldCondition(\n",
    "                    key=\"parent_asin\",\n",
    "                    match=MatchValue(value=parent_asin)\n",
    "                )\n",
    "            ]\n",
    "        ),\n",
    "        limit=100,\n",
    "        with_payload=True,\n",
    "        with_vectors=False\n",
    "    )[0]\n",
    "    \n",
    "    return points[0].payload[\"description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9c4db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_description(\"B0CBMPG524\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ea319d",
   "metadata": {},
   "source": [
    "# Create Eval dataset in Langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06lauxfamybp",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(api_key=os.environ[\"LANGSMITH_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623z7g3qu9j",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"rag-evaluation-dataset\"\n",
    "\n",
    "# Try to create dataset, if it already exists, read the existing one\n",
    "try:\n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        description=\"Dataset for evaluating RAG pipeline\"\n",
    "    )\n",
    "    print(f\"Created new dataset: {dataset_name}\")\n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e):\n",
    "        dataset = client.read_dataset(dataset_name=dataset_name)\n",
    "        print(f\"Using existing dataset: {dataset_name}\")\n",
    "    else:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "um1r02l6q5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in json_output:\n",
    "    client.create_example(\n",
    "        dataset_id=dataset.id,\n",
    "        inputs={\"question\": item[\"question\"]},\n",
    "        outputs={\n",
    "            \"ground_truth\": item[\"answer_example\"],\n",
    "            \"reference_context_ids\": item[\"chunk_ids\"],\n",
    "            \"reference_descriptions\": [get_description(id) for id in item[\"chunk_ids\"]]\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2199215",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
